---
title: "Running ICBM"
author: "Francis Durnin-Vermette"
date: "2022/12/19"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  html_notebook:
    toc: true
    number_sections: true
    df_print: paged
bibliography: bib.bib
editor_options:
  chunk_output_type: inline
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "doc") })
---

```{r include=FALSE}

library(ggplot2)
library(knitr)
library(dplyr)
library(geojson)
library(tibble)
library(purrr)
library(stringr)
library(here)
library(lubridate)
library(tidyr)

dir.create(tempdir()) #This fixes a bug if the temporary directory is not found

here::i_am("walkthrough_icbm.rmd")
source(here::here("r/icbm_calculate_re.r"))
source(here::here("r/icbm_run.r"))
source(here::here("r/read_climate_data.r"))
source(here::here("r/spinup.r"))
source(here::here("r/bayesian_gsa.r"))
source(here::here("r/bayesian_sir.r"))
source(here::here("r/bayesian_uncertainty.r"))


```

This walkthrough will go over reading input data,
grouping input data based on multiple sites or treatments,
running the models,
performing sensitivity analysis (global and local) using the models,
and performing parameter optimization using the models.

# Input data

## Site data
In order to run the models on input data, the site data needs to be grouped and
split into individual treatments with no repeated years.
First we show how to read the site data, then how to group and split it appropriately.

### Reading
We need to add SLC polygon IDs to the site data because this is how climate data is linked to site data.
```{r}
# read SLC polygon IDs
polyids_all <- readr::read_csv(here("data/climate_data","Documentation", "SLC_PCPTREGION.csv"))
# read site data
sitedata_all <- readr::read_csv(here("data/all_experiments_dummy.csv")) %>%
	left_join(polyids_all, by=c("latitude" = "Lat", "longitude" = "Long"))
```

### Single site
For this example, we filter out one treatment from the experiment conducted at the Ellerslie site.
```{r}
sitedata_ex_onesite <- sitedata_all %>%
	filter(location_name == "Ellerslie") %>%
	filter(treatment_number == 7) %>%
	filter(year_name <= 1985)
```

### Multiple treatments on one site
In this example, we want to simulate every treatment at the Ellerslie site.
In order to do this, we `group_by` treatment, and then split each of the groups into a seperate table using `group_split`.
This leaves us with a list of site data tables that we will pass iteratively into our modelling function.

```{r}
sitedata_ex_multipletrmt <- sitedata_all %>%
	filter(year_name <= 1985) %>%
	filter(location_name == "Ellerslie") %>%
	group_by(treatment_name) %>%
	group_split
```

### Multiple sites / teams
Data from specific locations or for specific teams and can be acquired using `filter`:
```{r}
sitedata_ex_multiplesite <- sitedata_all %>%
	filter(year_name > 1990) %>%
	filter(year_name < 2005) %>%
	filter(team_name == "A") %>%
	filter(location_name %in% c("Ellerslie", "Harrow")) %>%
	group_by(location_name, treatment_name, replication_number) %>%
	group_split

```

## Climate data
To run ICBM with the r~e~ calculator, the climate data should have these mandatory columns: `Year`, `JulianDay`, `Tavg`, which is the mean daily air temperature, `PREC`, which is the mean daily precipitation, and `PET`, which is the mean daily PET.

We can use the function `read_climate_data()` to fetch the relevant climate data, and format it according to the above requirements.

Note: use `suppressWarnings()` to run a function without any warnings.

```{r}
climate_data <- read_climate_data(climate_data_directory = here("data/climate_data","W9param_TablesCleaned"))
climate_data
```


# Running the models

## ICBM
ICBM simulations can be run using the function `run_icbm()` from the script `run_icbm_and_re.r`.
These simulations include the r~e~ parameter calculated from daily climate data.
The definitions of every input are given in the header of that script.
The function returns a similar output as the original ICBM function `icbm_holos4_classic_manure()`:
a table with ICBM C pools for every year, as well as the yearly mean r~e~ value.

### Single site
To run the model, we can simply pass the site and climate data to `run_icbm()`.
`ag_init`, `bg_init`, and `o_init` are initial C contents of those three pools,
and `irrigation_use_estimate` is a TRUE/FALSE value indicating whether you want daily irrigation values to be estimated based on site location.

```{r}
head(sitedata_ex_onesite, 3)
```


```{r warning=F}
result_ex_onesite <- run_icbm(
	SiteDataTable = sitedata_ex_onesite,
	DailyClimateTable = climate_data,
	ag_init = 0,
	bg_init = 0,
	o_init = 0)
result_ex_onesite
```

### Multiple treatments on the same site
We use the `map` function (from the library `purrr`) to pass each list element of `sitedata_ex_multipletrmt` into `run_icbm`.
More information about how `purrr` and `map` work can be found here: https://purrr.tidyverse.org/reference/map.html

```{r}
sitedata_ex_multipletrmt[1:3] %>%
	map(~head(.,3))
```


```{r warning=F}

result_ex_multipletrmt <- sitedata_ex_multipletrmt %>%
	suppressWarnings() %>%
	map(function(x) {
		run_icbm(
			SiteDataTable = x,
			DailyClimateTable = climate_data,
			ag_init = 0,
			bg_init = 0,
			o_init = 0)
		
	}
	)

```

```{r}
result_ex_multipletrmt[1:3] %>%
	map(~head(.,3))
```

In the `map` function, `~` is shorthand for defining the function that we will iterate through, and `.` refers to the current element of the iteration.
Using these shorthands, we can simplify this process:
```{r warning=F}
result_ex_multipletrmt <- sitedata_ex_multipletrmt %>%
	map(~run_icbm(
		SiteDataTable = .,
		DailyClimateTable = climate_data,
		ag_init = 0,
		bg_init = 0,
		o_init = 0))
```


### Multiple sites
```{r}
sitedata_ex_multiplesite[21:23] %>%
	map(~head(.,3))
```

```{r warning=F}
result_ex_multiplesite <- sitedata_ex_multiplesite %>%
	map(~run_icbm(
		SiteDataTable = .,
		DailyClimateTable = climate_data,
		ag_init = 0,
		bg_init = 0,
		o_init = 0))
```

```{r}
result_ex_multiplesite[21:23] %>%
	map(~head(.,3))
```


# Grouping errors
Each input to `run_icbm()` is a seperate model run, and so needs to have consecutive years.
Otherwise, an error is returned:
```{r}
sitedata_ex_error1 <- sitedata_all %>%
	filter(team_name == "B") %>%
	filter(year_name >= 1981) %>%
	group_by(location_name) %>%
	group_split
```

```{r}
try(sitedata_ex_error1 %>%
			map(~run_icbm(
				SiteDataTable = .,
				DailyClimateTable = climate_data,
				ag_init = 0,
				bg_init = 0,
				o_init = 0)))
```

We can see the problem better by finding duplicated years in our list elements:

```{r}
sitedata_ex_error1 %>%
	map(~any(duplicated(.$year_name))) %>%
	unlist
```
This shows that in both of our groups, there are duplicated years, which is an invalid input to `run_icbm`.
In contrast, in one of our previous examples there are no instances of duplicated years:
```{r}
sitedata_ex_multiplesite %>%
	map(~any(duplicated(.$year_name))) %>%
	unlist
```

This problem can be fixed by changing our site data groupings.
We need to make sure our groups encompass every permutation of the experiments we are looking at.
In the examples above, we only needed to add location_name, treatment_name, and replication_number in order to achieve valid groupings.
However, this does not work for all sites:
```{r}
sitedata_ex_error2 <- sitedata_all %>%
	filter(team_name == "B") %>%
	filter(year_name >= 1981) %>%
	group_by(team_name, location_name, treatment_name, replication_number) %>%
	group_split
sitedata_ex_error2 %>%
	map(~any(duplicated(.$year_name))) %>%
	unlist

```

In these sites, there are actually two other grouping variables that we need to account for: soil depth and plot id.

In our dataset, soil_depth_min_cm and soil_depth_max_cm are in separate columns.
To create a grouping variable for soil depth, we concatenate these two columns into a single soil depth column, and then add this as a new group.

Then, by adding soil_depth as well as plot_id to our grouping variables, we arrive at valid inputs to the model:
```{r}
sitedata_ex_depth <- sitedata_all %>%
	filter(team_name == "B") %>%
	filter(year_name >= 1981) %>%
	mutate(soil_depth = paste0(soil_depth_min_cm,"-",soil_depth_max_cm)) %>%
	group_by(team_name, location_name, treatment_name, replication_number, plot_id, soil_depth) %>%
	group_split
sitedata_ex_depth %>%
	map(~any(duplicated(.$year_name))) %>%
	unlist
```

```{r warning=F}
result_ex_depth <- sitedata_ex_depth[1:10] %>%
	map(~run_icbm(
		SiteDataTable = .,
		DailyClimateTable = climate_data,
		ag_init = 0,
		bg_init = 0,
		o_init = 0))
```
```{r}
result_ex_depth[1:3] %>%
	map(~head(.,3))
```

Other possible columns that could lead to problems with grouping include `field_name`, `block`, etc.

# Parameter overrides
Certain parameters need to be overridden in order to perform calibrations or sensitivity analyses.
This can be done by passing the parameters to override directly into the `run_icbm` function.

Constant parameters (e.g. `SoilTopThickness`, default = 250) and parameters that are inputted as part of an external data table (e.g. `PREC` from the climate data table, no default) can both be overridden.

```{r warning=F}
result_ex_multipletrmt_override <- sitedata_ex_multipletrmt %>%
	map(~run_icbm(
		SiteDataTable = .,
		DailyClimateTable = climate_data,
		ag_init = 0,
		bg_init = 0,
		o_init = 0,
		# overrides
		hag = 0.1))
```

```{r}
tibble(Year = result_ex_multipletrmt[[1]]$time,
			 `Total SOC Regular` = result_ex_multipletrmt[[1]]$Tot,
			 `Total SOC Overridden` = result_ex_multipletrmt_override[[1]]$Tot)

```


# Spin-up

To perform the spin-up, we calculate the proportion of SOC pool sizes at steady state (SS) for the first 10 years of experimental input data.
We can either do this by run the simulation for a long time (10,000 years) until it reaches a steady state, or, if the model calculates SS as part of its operation (such as the IPCC Tier 2 Steady State model), we can just use those calculations of SS.

The `spinup` function takes site data, averages it, and uses this to compute the SS for each pool of the chosen model.
It uses the initial C value from the input data to convert proportional SS pool sizes into SOC pool sizes, and returns these as a list so that they can be easily passed to another modelling function.

## One site
```{r}
spinup_sitedata_ex_onesite <- sitedata_ex_onesite %>%
	slice(1:10)

spinup_ex_onesite <- spinup(site_data = spinup_sitedata_ex_onesite,
			 climate_data = climate_data,
			 initial_c = 1000,
			 model = "ipcct2")

spinup_ex_onesite
```

```{r}
run_ipcct2(
	site_data = sitedata_ex_onesite,
	climate_data = climate_data,
	init_active = spinup_ex_onesite$init_active,
	init_slow = spinup_ex_onesite$init_slow,
	init_passive = spinup_ex_onesite$init_passive)
```

## Multiple sites with multiple initial C values
```{r}
spinup_sitedata_ex_multipletrmt <- sitedata_ex_multipletrmt %>%
	map(~slice(., 1:10))

# Note: this is just an example. The column total_yield has nothing to do with initial C values.
initial_c_ex_multipletrmt <- sitedata_ex_multipletrmt %>%
	map(~.$total_yield[1])
initial_c_ex_multipletrmt

spinup_ex_multipletrmt <- map2(.x = spinup_sitedata_ex_multipletrmt,
															 .y = initial_c_ex_multipletrmt,
															 ~spinup(site_data = .x, 
															 				climate_data = climate_data,
															 				initial_c = .y,
															 				model = "ipcct2"))
spinup_ex_multipletrmt
```

And now we run the model by iterating through the site_data and initial_c lists simultaneously.
```{r}
sitedata_ex_multipletrmt %>%
	map2(spinup_ex_multipletrmt,
			 ~run_ipcct2(site_data = .x,
			 						climate_data = climate_data,
			 						initial_c = .y))
```



# Sensitivity analysis
```{r}
param_bounds <- read.csv(here("data/ipcct2_parameters_gsa.csv"), stringsAsFactors = FALSE)
param_bounds
```

```{r}
sa_ex_onesite_sj <- gsa(site_data = sitedata_ex_onesite,
										 climate_data = climate_data,
										 initial_c = spinup_ex_onesite,
										 parameter_bounds = param_bounds,
										 sample_size = 10,
										 method = "soboljansen")

sa_ex_onesite_sj
```
The fast99 method of computing sensitivity analyses indices can also be used
```{r}
sa_ex_onesite_f99 <- gsa(site_data = sitedata_ex_onesite,
										 climate_data = climate_data,
										 initial_c = spinup_ex_onesite,
										 parameter_bounds = param_bounds,
										 sample_size = 100,
										 method = "fast99")

sa_ex_onesite_f99

ggplot(sa_ex_onesite_f99)+
	geom_bar(aes(x=reorder(params, interactions), y=main),stat='identity', 
					 fill="black") +
	geom_bar(aes(x=reorder(params, interactions), y=interactions),
					 stat='identity', fill="grey",alpha=0.7 ) +
	coord_flip() +
	theme_bw()
```


The sensitivity analysis can also be run on a list of multiple sites/treatments.
This can be done by passing a list of data.frames to the function.
In this case, each site/treatment is passed the same sets of parameters,
and the results represent the mean value between each site/treatment.

```{r}
sa_ex_multipletrmt <- gsa(site_data = sitedata_ex_multipletrmt,
													climate_data = climate_data,
													initial_c = spinup_ex_multipletrmt,
													parameter_bounds = param_bounds,
													sample_size = 10)

sa_ex_multipletrmt
```
Results of the sensitivity analysis can be graphed
```{r}
# First-order sensitivity
ggplot(sa_ex_multipletrmt, aes(x = reorder(params, -singsi), y = singsi, ymax=singsi.uci, ymin=singsi.lci)) + 
	xlab("Parameters") +
	ylab("First-Order Sensitivity Index") +
	geom_errorbar(width=0.2, size=1, color="black") +
	geom_bar(stat='identity', fill="grey", alpha=0.70 ) +
	coord_flip() +
	theme_bw()

# Total order sensitivity
ggplot(sa_ex_multipletrmt, aes(x = reorder(params, -singsi), y = totsi, ymax=totsi.uci, ymin=totsi.lci))+ 
	xlab("Parameters") +
	ylab("Total-Order Sensitivity Index") + 
	geom_errorbar(width=0.2, size=1, color="black")+
	geom_bar(stat='identity', fill="grey", alpha=0.70) +
	coord_flip() +
	theme_bw()

# Both
ggplot(sa_ex_multipletrmt, aes(x = reorder(params, -singsi), y = totsi, ymax=totsi.uci, ymin=totsi.lci)) +
        geom_bar(aes(y = singsi), stat='identity', fill="black", alpha=0.7)+
        geom_bar(aes(y = totsi), stat='identity', fill="grey", alpha=0.7)+
        geom_errorbar(width=0.2, size=1)+
        coord_flip() +
			 	theme_bw()

```
# Sampling Importance Resampling (SIR)
Following the sensitivity analysis, we select the parameters that have the highest sensitivities for use in our Bayesian calibration.
@gurungBayesianCalibrationDayCent2020 use a 2.5% cut-off value for sensitivity. Parameters above this level are considered "sensitive parameters" and are included in the SIR, otherwise they are excluded.
```{r}
param_bounds
sa_ex_onesite_sj
```


SIR can also be run using a list of site data.frames:
```{r}
sir_ex_multipletrmt <- sir(site_data = sitedata_ex_multipletrmt,
													climate_data = climate_data,
													initial_c = spinup_ex_multipletrmt,
													parameter_bounds = param_bounds,
													sample_size = 100,
													resample_size = 10)
sir_ex_multipletrmt
```

```{r}
prior <- sir_ex_multipletrmt$prior %>%
	pivot_longer(cols = !`SampleID`)
posterior <- sir_ex_multipletrmt$posterior %>%
	pivot_longer(cols = !`SampleID`)

ggplot() +
	geom_density(data = posterior, aes(value), col = NA, fill = "red", alpha = 0.2) +
	geom_density(data = prior, aes(value), col = NA, fill = "blue", alpha = 0.2) +
	facet_wrap(~name, scales = "free", ncol = 3) +
	theme_bw() +
	theme(panel.border = element_rect(colour = "black", fill = NA), panel.grid.major = element_blank(),
				axis.title.x=element_blank(),panel.grid.minor = element_blank()) +
	scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))
```

# Uncertainty analysis
```{r}
uncertainty_ex_multipletrmt <- montecarlo(site_data = sitedata_ex_multipletrmt,
																					climate_data = climate_data,
																					initial_c = spinup_ex_multipletrmt,
																					distribution = sir_ex_multipletrmt$posterior,
																					sample_size = 10)
uncertainty_ex_multipletrmt
```
We can graph this.
```{r}
ggplot(data = NULL, aes(x=soc_total/10, y=soc_tha*(30/(soil_depth_max_cm - soil_depth_min_cm)))) +
	geom_point(data=uncertainty_ex_multipletrmt$montecarlo, color = "grey") +
	geom_point(data=uncertainty_ex_multipletrmt$median, color="black") +
	geom_abline(intercept = 0, slope = 1) +
	coord_fixed(ratio = 1, xlim = c(0,400), ylim = c(0,400), expand = TRUE, clip="off") +
	theme_bw()
```


# Appendix

## Parameter overrides: optimizing a parameter using `optim`
To demonstrate a practical example of using parameter overrides, we will use the built-in `optim` function.

`optim` passes a vector to the function you define, and finds the values that minimize this function.
The vector can have names, and these names are also passed to the function, which allows us to override parameters based on name.

```{r}
ex_func <- function(parameters) {
	parameters["x"] + parameters["y"]
}
optim(par = c(x=-1.2,y=1), fn = ex_func, method = "L-BFGS-B", lower = c(x=0,y=1), upper = c(x=1,y=5))
```

In order to use optimization functions such as this with SOC models, we usually need to write a small wrapper function that translates inputs/outputs of our model script for use by the optimization function.
In this case, the parameters that we are interested in optimizing are passed as the first argument of the function.

This function runs the model, finds the mean r~e~ between years, and returns that value.
Therefore, by passing this function to `optim`, we will be asking it to minimize the value of r~e~.
In practice, you will want to write a wrapper function that computes model error.

```{r}
icbm_wrapper_example <- function(parameters,
																 DailyClimateTable,
																 SiteDataTable,
																 ag_init = 0,
																 bg_init = 0,
																 o_init = 0,
																 ...) {
	args <- list(...)
	
	inputs <- list(DailyClimateTable = DailyClimateTable,
								 SiteDataTable = SiteDataTable,
								 ag_init = ag_init,
								 bg_init = bg_init,
								 o_init = o_init) %>%
		append(args) %>%
		append(as.list(parameters))
	
	model_results <- do.call(run_icbm,inputs)
	
	mean_re <- mean(bind_rows(model_results)$re)
	
	print(paste0(names(parameters),": ",parameters))
	print(paste0("mean_re: ",mean_re))
	return(mean_re)
}
```
We can override a parameter using the first argument by hand, or pass the whole function into the `optim` function.
```{r}
icbm_wrapper_example(
	parameters = c(r_c = 1),
	SiteDataTable = sitedata_ex_multipletrmt[[1]],
	DailyClimateTable = climate_data,
	ag_init = 0,
	bg_init = 0,
	o_init = 0
)

optim(par = c(r_c = 0.5), fn = icbm_wrapper_example, method = "L-BFGS-B", lower = c(r_c = 0), upper = c(r_c = 1), SiteDataTable = sitedata_ex_multipletrmt[[1]], DailyClimateTable = climate_data)
```
We can optimize for multiple parameters
```{r}
optim(par = c(r_c = 0.5, hag = 0.1), fn = icbm_wrapper_example, method = "L-BFGS-B", lower = c(r_c = 0, hag = 0), upper = c(r_c = 1, hag = 1), SiteDataTable = sitedata_ex_multipletrmt[[1]], DailyClimateTable = climate_data)
```

```{r include=FALSE}
#Debug
SiteDataTable = sitedata_ex_onesite
DailyClimateTable = climate_data
ag_init = 0
bg_init = 0
o_init = 0
irrigation_use_estimate = FALSE

alfa = 0.7
SoilTopThickness = 250
Temp_min = -3.78
Temp_max = 30
r_s = 0.42
r_wp = 0.18
ReferenceAdjustment = 0.10516

r_c = NA
tillage_soil = "Brown"
tillage_type = "Intensive Tillage"
irrigation_region = "Canada"
irrigation_use_estimate = FALSE
irrigation = 0

yield = filter(SiteDataTable, year_name == 1983)$total_yield
perennial = filter(SiteDataTable, year_name == 1983)$perennial
SoilOrganicC_Percent = filter(SiteDataTable, year_name == 1983)$soil_total_carbon_px
ClayContent = filter(SiteDataTable, year_name == 1983)$clay_px
SandContent = filter(SiteDataTable, year_name == 1983)$sand_px
```

# References
